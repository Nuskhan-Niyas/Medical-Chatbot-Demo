{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4bbba85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face model cache directory: /Users/nuzkhan/.cache/huggingface/transformers\n"
     ]
    }
   ],
   "source": [
    "from transformers.utils import logging\n",
    "from pathlib import Path\n",
    "\n",
    "cache_dir = Path.home() / \".cache/huggingface/transformers\"\n",
    "print(\"Hugging Face model cache directory:\", cache_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87726ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /Users/nuzkhan/Downloads/medical chatbot/medchatbot/lib/python3.9/site-packages (0.12.0)\n",
      "Requirement already satisfied: protobuf in /Users/nuzkhan/Downloads/medical chatbot/medchatbot/lib/python3.9/site-packages (6.33.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/nuzkhan/Downloads/medical chatbot/medchatbot/lib/python3.9/site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/nuzkhan/Downloads/medical chatbot/medchatbot/lib/python3.9/site-packages (from tiktoken) (2025.11.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/nuzkhan/Downloads/medical chatbot/medchatbot/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nuzkhan/Downloads/medical chatbot/medchatbot/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nuzkhan/Downloads/medical chatbot/medchatbot/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (2025.10.5)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nuzkhan/Downloads/medical chatbot/medchatbot/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/Users/nuzkhan/Downloads/medical chatbot/medchatbot/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#! pip install tiktoken protobuf\n",
    "#! pip install torch transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c333001",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Downloads/medical chatbot/medchatbot/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:1780\u001b[0m, in \u001b[0;36mconvert_slow_tokenizer\u001b[0;34m(transformer_tokenizer, from_tiktoken)\u001b[0m\n\u001b[1;32m   1779\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting from Tiktoken\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTikTokenConverter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1783\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1784\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[0;32m~/Downloads/medical chatbot/medchatbot/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:1677\u001b[0m, in \u001b[0;36mTikTokenConverter.converted\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconverted\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tokenizer:\n\u001b[0;32m-> 1677\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1678\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mpre_tokenizer \u001b[38;5;241m=\u001b[39m pre_tokenizers\u001b[38;5;241m.\u001b[39mSequence(\n\u001b[1;32m   1679\u001b[0m         [\n\u001b[1;32m   1680\u001b[0m             pre_tokenizers\u001b[38;5;241m.\u001b[39mSplit(Regex(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpattern), behavior\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124misolated\u001b[39m\u001b[38;5;124m\"\u001b[39m, invert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1681\u001b[0m             pre_tokenizers\u001b[38;5;241m.\u001b[39mByteLevel(add_prefix_space\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space, use_regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1682\u001b[0m         ]\n\u001b[1;32m   1683\u001b[0m     )\n",
      "File \u001b[0;32m~/Downloads/medical chatbot/medchatbot/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:1670\u001b[0m, in \u001b[0;36mTikTokenConverter.tokenizer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtokenizer\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 1670\u001b[0m     vocab_scores, merges \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_vocab_merges_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(BPE(vocab_scores, merges, fuse_unk\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[0;32m~/Downloads/medical chatbot/medchatbot/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:1646\u001b[0m, in \u001b[0;36mTikTokenConverter.extract_vocab_merges_from_model\u001b[0;34m(self, tiktoken_url)\u001b[0m\n\u001b[1;32m   1642\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1643\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tiktoken` is required to read a `tiktoken` file. Install it with `pip install tiktoken`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1644\u001b[0m     )\n\u001b[0;32m-> 1646\u001b[0m bpe_ranks \u001b[38;5;241m=\u001b[39m \u001b[43mload_tiktoken_bpe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtiktoken_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1647\u001b[0m byte_encoder \u001b[38;5;241m=\u001b[39m bytes_to_unicode()\n",
      "File \u001b[0;32m~/Downloads/medical chatbot/medchatbot/lib/python3.9/site-packages/tiktoken/load.py:162\u001b[0m, in \u001b[0;36mload_tiktoken_bpe\u001b[0;34m(tiktoken_bpe_file, expected_hash)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_tiktoken_bpe\u001b[39m(tiktoken_bpe_file: \u001b[38;5;28mstr\u001b[39m, expected_hash: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# NB: do not add caching to this function\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m     contents \u001b[38;5;241m=\u001b[39m \u001b[43mread_file_cached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtiktoken_bpe_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_hash\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     ret \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/Downloads/medical chatbot/medchatbot/lib/python3.9/site-packages/tiktoken/load.py:52\u001b[0m, in \u001b[0;36mread_file_cached\u001b[0;34m(blobpath, expected_hash)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m read_file(blobpath)\n\u001b[0;32m---> 52\u001b[0m cache_key \u001b[38;5;241m=\u001b[39m hashlib\u001b[38;5;241m.\u001b[39msha1(\u001b[43mblobpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m())\u001b[38;5;241m.\u001b[39mhexdigest()\n\u001b[1;32m     54\u001b[0m cache_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(cache_dir, cache_key)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'encode'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m model_path \u001b[38;5;241m=\u001b[39m Path\u001b[38;5;241m.\u001b[39mhome() \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistral_models\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m7B-Instruct-v0.3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Use trust_remote_code=True to let Transformers load Mistral custom tokenizer\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     10\u001b[0m     model_path,\n\u001b[1;32m     11\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[1;32m     13\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Test generation\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/medical chatbot/medchatbot/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:1159\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1156\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[38;5;241m=\u001b[39m TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1161\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Downloads/medical chatbot/medchatbot/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2097\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2094\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2095\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2097\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2099\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2100\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2101\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2105\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2106\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2109\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/medical chatbot/medchatbot/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2343\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2341\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2343\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2344\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[1;32m   2345\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2346\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2347\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2348\u001b[0m     )\n",
      "File \u001b[0;32m~/Downloads/medical chatbot/medchatbot/lib/python3.9/site-packages/transformers/models/llama/tokenization_llama_fast.py:154\u001b[0m, in \u001b[0;36mLlamaTokenizerFast.__init__\u001b[0;34m(self, vocab_file, tokenizer_file, clean_up_tokenization_spaces, unk_token, bos_token, eos_token, add_bos_token, add_eos_token, use_default_system_prompt, legacy, add_prefix_space, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m add_prefix_space \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_slow\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43munk_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munk_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43meos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_bos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_bos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_eos_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_eos_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_default_system_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_default_system_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_prefix_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_prefix_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlegacy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlegacy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_bos_token \u001b[38;5;241m=\u001b[39m add_bos_token\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_eos_token \u001b[38;5;241m=\u001b[39m add_eos_token\n",
      "File \u001b[0;32m~/Downloads/medical chatbot/medchatbot/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:139\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocab_file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_special_tokens \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditional_special_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n\u001b[0;32m--> 139\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_slow_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tiktoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     slow_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Downloads/medical chatbot/medchatbot/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:1785\u001b[0m, in \u001b[0;36mconvert_slow_tokenizer\u001b[0;34m(transformer_tokenizer, from_tiktoken)\u001b[0m\n\u001b[1;32m   1780\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TikTokenConverter(\n\u001b[1;32m   1781\u001b[0m         vocab_file\u001b[38;5;241m=\u001b[39mtransformer_tokenizer\u001b[38;5;241m.\u001b[39mvocab_file,\n\u001b[1;32m   1782\u001b[0m         additional_special_tokens\u001b[38;5;241m=\u001b[39mtransformer_tokenizer\u001b[38;5;241m.\u001b[39madditional_special_tokens,\n\u001b[1;32m   1783\u001b[0m     )\u001b[38;5;241m.\u001b[39mconverted()\n\u001b[1;32m   1784\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m-> 1785\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1786\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1787\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith a SentencePiece tokenizer.model file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1788\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently available slow->fast converters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(SLOW_TO_FAST_CONVERTERS\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1789\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "model_path = Path.home() / \"mistral_models\" / \"7B-Instruct-v0.3\"\n",
    "\n",
    "# Use trust_remote_code=True to let Transformers load Mistral custom tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Test generation\n",
    "prompt = \"Explain COVID-19 in 2 sentences.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe1afae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d08787a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a1978be",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "787058e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(76293) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "Fetching 10 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  5.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Gemma 3 model downloaded to: /Users/nuzkhan/gemma_models/gemma-3-1b-it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "# Path where Gemma will be saved\n",
    "gemma_path = Path.home() / \"gemma_models\" / \"gemma-3-1b-it\"\n",
    "gemma_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download the Gemma 3 instruction-tuned 1B model\n",
    "snapshot_download(\n",
    "    repo_id=\"google/gemma-3-1b-it\",  # Gemma 3 1B instruction-tuned\n",
    "    local_dir=gemma_path,\n",
    "    allow_patterns=[\"*\"]  # download all files\n",
    ")\n",
    "\n",
    "print(f\"âœ… Gemma 3 model downloaded to: {gemma_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c396f851",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f10dc339",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… LLM loaded successfully\n",
      "Generated text: Explain what a fever is in 2 sentences.\n",
      "\n",
      "A fever is a temporary increase in body temperature caused by an infection or other illness. It's a sign that your body is fighting off harmful substances.\n",
      "\n",
      "---\n",
      "\n",
      "**Here are some additional points to consider:**\n",
      "\n",
      "*   **Not always\n"
     ]
    }
   ],
   "source": [
    "from app.components.llm import load_llm\n",
    "import torch\n",
    "\n",
    "tokenizer, model = load_llm()\n",
    "if tokenizer is None or model is None:\n",
    "    print(\"âŒ Failed to load LLM\")\n",
    "else:\n",
    "    print(\"âœ… LLM loaded successfully\")\n",
    "\n",
    "# Quick test generation\n",
    "prompt = \"Explain what a fever is in 2 sentences.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50, temperature=0.3)\n",
    "\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated text:\", text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e28d0a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever.py\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "from app.components.llm import load_llm\n",
    "from app.components.vector_store import load_vector_store, save_vector_store\n",
    "from app.components.pdf_loader import load_pdf_files, create_text_chunks\n",
    "from app.components.embeddings import get_embedding_model\n",
    "from app.config.config import DB_FAISS_PATH\n",
    "from app.common.logger import get_logger\n",
    "from app.common.custom_exception import CustomException\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "CUSTOM_PROMPT_TEMPLATE = \"\"\" Answer the following medical question in 2-3 lines maximum using only the information provided in the context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "def set_custom_prompt():\n",
    "    return PromptTemplate(\n",
    "        template=CUSTOM_PROMPT_TEMPLATE,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "\n",
    "def create_qa_chain():\n",
    "    try:\n",
    "        logger.info(\"Loading vector store for context...\")\n",
    "        db = load_vector_store()\n",
    "\n",
    "        # If vector store is missing, create it automatically\n",
    "        if db is None:\n",
    "            logger.warning(\"Vector store not found. Creating a new one...\")\n",
    "            documents = load_pdf_files()\n",
    "            if not documents:\n",
    "                raise CustomException(\"No documents found to create vector store\")\n",
    "            text_chunks = create_text_chunks(documents)\n",
    "            db = save_vector_store(text_chunks)\n",
    "            if db is None:\n",
    "                raise CustomException(\"Failed to create vector store\")\n",
    "\n",
    "        # Load Gemma 3 LLM\n",
    "        tokenizer, llm_model = load_llm()\n",
    "        if not tokenizer or not llm_model:\n",
    "            raise CustomException(\"LLM not loaded\")\n",
    "\n",
    "        # Define a simple wrapper function for Gemma 3 to integrate with LangChain\n",
    "        class GemmaWrapper:\n",
    "            def __init__(self, tokenizer, model):\n",
    "                self.tokenizer = tokenizer\n",
    "                self.model = model\n",
    "\n",
    "            def __call__(self, prompt: str):\n",
    "                inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model.generate(**inputs, max_new_tokens=256)\n",
    "                return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        llm = GemmaWrapper(tokenizer, llm_model)\n",
    "\n",
    "        # Create the RetrievalQA chain\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=db.as_retriever(search_kwargs={'k': 1}),\n",
    "            return_source_documents=False,\n",
    "            chain_type_kwargs={'prompt': set_custom_prompt()}\n",
    "        )\n",
    "\n",
    "        logger.info(\"âœ… QA chain created successfully\")\n",
    "        return qa_chain\n",
    "\n",
    "    except Exception as e:\n",
    "        error_message = CustomException(\"Failed to create QA chain\", e)\n",
    "        logger.error(str(error_message))\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7d5554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05646383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema import BaseRetriever\n",
    "\n",
    "from app.components.llm import load_llm\n",
    "from app.components.vector_store import load_vector_store, save_vector_store\n",
    "from app.components.pdf_loader import load_pdf_files, create_text_chunks\n",
    "from app.common.logger import get_logger\n",
    "from app.common.custom_exception import CustomException\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "CUSTOM_PROMPT_TEMPLATE = \"\"\" Answer the following medical question in 2-3 lines maximum using only the information provided in the context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "def set_custom_prompt():\n",
    "    return PromptTemplate(\n",
    "        template=CUSTOM_PROMPT_TEMPLATE,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "class GemmaLLMWrapper:\n",
    "    \"\"\"Wrap Gemma 3 for LangChain.\"\"\"\n",
    "    def __init__(self, tokenizer, model):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, prompt: str, **kwargs):\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(**inputs, max_new_tokens=kwargs.get(\"max_new_tokens\", 256))\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def create_qa_chain():\n",
    "    try:\n",
    "        logger.info(\"Loading vector store for context...\")\n",
    "        db = load_vector_store()\n",
    "\n",
    "        # Auto-create vectorstore if missing\n",
    "        if db is None:\n",
    "            logger.warning(\"Vector store not found. Creating a new one...\")\n",
    "            documents = load_pdf_files()\n",
    "            if not documents:\n",
    "                raise CustomException(\"No documents found to create vector store\")\n",
    "            text_chunks = create_text_chunks(documents)\n",
    "            db = save_vector_store(text_chunks)\n",
    "            if db is None:\n",
    "                raise CustomException(\"Failed to create vector store\")\n",
    "\n",
    "        # Load Gemma 3\n",
    "        tokenizer, llm_model = load_llm()\n",
    "        if not tokenizer or not llm_model:\n",
    "            raise CustomException(\"LLM not loaded\")\n",
    "\n",
    "        llm = GemmaLLMWrapper(tokenizer, llm_model)\n",
    "\n",
    "        # Ensure retriever implements BaseRetriever\n",
    "        retriever: BaseRetriever = db.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever,\n",
    "            return_source_documents=False,\n",
    "            chain_type_kwargs={\"prompt\": set_custom_prompt()},\n",
    "        )\n",
    "\n",
    "        logger.info(\"âœ… QA chain created successfully\")\n",
    "        return qa_chain\n",
    "\n",
    "    except Exception as e:\n",
    "        error_message = CustomException(\"Failed to create QA chain\", e)\n",
    "        logger.error(str(error_message))\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfe1991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394f1e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37899052",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nuzkhan/Downloads/medical chatbot/medchatbot/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/nuzkhan/Downloads/medical chatbot/medchatbot/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Local path to Gemma 3 model\n",
    "LOCAL_GEMMA_PATH = Path.home() / \"Downloads\" / \"medical_chatbot\" / \"gemma_models\" / \"gemma-3-1b-it\"\n",
    "\n",
    "def load_llm(local_path: Path = LOCAL_GEMMA_PATH):\n",
    "    \"\"\"\n",
    "    Load Gemma 3 LLM from a local path.\n",
    "    Returns: tokenizer, model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not local_path.exists():\n",
    "            raise FileNotFoundError(f\"Local model path does not exist: {local_path}\")\n",
    "\n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            local_path,\n",
    "            use_fast=False,\n",
    "            trust_remote_code=True,\n",
    "            repo_type=\"model\",\n",
    "            local_files_only=True\n",
    "        )\n",
    "\n",
    "        # Load model\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            local_path,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            trust_remote_code=True,\n",
    "            repo_type=\"model\",\n",
    "            local_files_only=True\n",
    "        ).eval()\n",
    "\n",
    "        return tokenizer, model\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to load LLM: {e}\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3d1e30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Failed to load LLM: __init__() got an unexpected keyword argument 'repo_type'\n",
      "âŒ Failed to load LLM\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Quick test in Jupyter\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    tokenizer, model = load_llm()\n",
    "    if tokenizer and model:\n",
    "        print(\"âœ… Gemma 3 LLM loaded successfully!\")\n",
    "\n",
    "        # Quick generation test\n",
    "        input_text = \"Explain what diabetes is in 2 sentences.\"\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "        result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(\"Input:\", input_text)\n",
    "        print(\"Output:\", result)\n",
    "    else:\n",
    "        print(\"âŒ Failed to load LLM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba77f248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python(8986) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.1-cp39-cp39-macosx_11_0_arm64.whl (1.3 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.3 MB 7.7 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.1\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/Users/nuzkhan/Downloads/medical chatbot/medchatbot/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install SentencePiece\n",
    "#! pip install sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacef3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b43679b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c842c98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… HF_TOKEN loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema import BaseRetriever\n",
    "\n",
    "from app.components.llm import load_llm\n",
    "from app.components.vector_store import load_vector_store, save_vector_store\n",
    "from app.components.pdf_loader import load_pdf_files, create_text_chunks\n",
    "from app.common.logger import get_logger\n",
    "from app.common.custom_exception import CustomException\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "CUSTOM_PROMPT_TEMPLATE = \"\"\" Answer the following medical question in 2-3 lines maximum using only the information provided in the context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "def set_custom_prompt():\n",
    "    return PromptTemplate(\n",
    "        template=CUSTOM_PROMPT_TEMPLATE,\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "    )\n",
    "\n",
    "class GemmaLLMWrapper:\n",
    "    \"\"\"Wrap Gemma 3 for LangChain.\"\"\"\n",
    "    def __init__(self, tokenizer, model):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "\n",
    "    def __call__(self, prompt: str, **kwargs):\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(**inputs, max_new_tokens=kwargs.get(\"max_new_tokens\", 256))\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def create_qa_chain():\n",
    "    try:\n",
    "        logger.info(\"Loading vector store for context...\")\n",
    "        db = load_vector_store()\n",
    "\n",
    "        # Auto-create vectorstore if missing\n",
    "        if db is None:\n",
    "            logger.warning(\"Vector store not found. Creating a new one...\")\n",
    "            documents = load_pdf_files()\n",
    "            if not documents:\n",
    "                raise CustomException(\"No documents found to create vector store\")\n",
    "            text_chunks = create_text_chunks(documents)\n",
    "            db = save_vector_store(text_chunks)\n",
    "            if db is None:\n",
    "                raise CustomException(\"Failed to create vector store\")\n",
    "\n",
    "        # Load Gemma 3\n",
    "        tokenizer, llm_model = load_llm()\n",
    "        if not tokenizer or not llm_model:\n",
    "            raise CustomException(\"LLM not loaded\")\n",
    "\n",
    "        llm = GemmaLLMWrapper(tokenizer, llm_model)\n",
    "\n",
    "        # Ensure retriever implements BaseRetriever\n",
    "        retriever: BaseRetriever = db.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "        qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever,\n",
    "            return_source_documents=False,\n",
    "            chain_type_kwargs={\"prompt\": set_custom_prompt()},\n",
    "        )\n",
    "\n",
    "        logger.info(\"âœ… QA chain created successfully\")\n",
    "        return qa_chain\n",
    "\n",
    "    except Exception as e:\n",
    "        error_message = CustomException(\"Failed to create QA chain\", e)\n",
    "        logger.error(str(error_message))\n",
    "        #return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00727a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb7914fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ProtocolError('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer')), '(Request ID: 1bc2536b-a168-4e6b-b71c-2b1f8a21a88a)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/./modules.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Vector store missing\n"
     ]
    }
   ],
   "source": [
    "from app.components.vector_store import load_vector_store\n",
    "\n",
    "db = load_vector_store()\n",
    "if db:\n",
    "    print(\"âœ… Vector store loaded successfully\")\n",
    "else:\n",
    "    print(\"âŒ Vector store missing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "634b2d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ QA chain failed\n"
     ]
    }
   ],
   "source": [
    "from app.components.retriever import create_qa_chain\n",
    "qa_chain = create_qa_chain()\n",
    "if qa_chain:\n",
    "    print(\"âœ… QA chain is ready\")\n",
    "else:\n",
    "    print(\"âŒ QA chain failed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fdb3bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Creating QA Chain...\n",
      "âŒ QA Chain creation failed.\n"
     ]
    }
   ],
   "source": [
    "from app.components.retriever import create_qa_chain\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ðŸ”„ Creating QA Chain...\")\n",
    "    qa_chain = create_qa_chain()\n",
    "    if qa_chain:\n",
    "        print(\"âœ… QA Chain created successfully!\")\n",
    "        query = \"What are the symptoms of diabetes?\"\n",
    "        print(\"ðŸ¤– Testing response:\")\n",
    "        response = qa_chain.invoke({\"query\": query})\n",
    "        print(\"ðŸ§  LLM Response:\\n\", response.get(\"result\", \"No result\"))\n",
    "    else:\n",
    "        print(\"âŒ QA Chain creation failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0710685",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fcc35b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response:\n",
      "What are the symptoms of diabetes?\n",
      "\n",
      "Diabetes symptoms can vary widely from person to person. Some people may have no symptoms at all, while others may experience significant symptoms. Here's a breakdown of common symptoms:\n",
      "\n",
      "*   **Frequent urination:** This is often the first sign of diabetes, as the body tries to get rid of excess sugar.\n",
      "*   **Excessive thirst:** High blood sugar levels can draw fluid from the body, leading to thirst.\n",
      "*   **Unexplained weight loss:**  The body can't use sugar for energy, leading to weight loss despite eating normally.\n",
      "*   **Increased hunger:**  The body isn't getting the energy it needs.\n",
      "*   **Blurry vision:** High blood sugar can affect the lens of the eye, causing blurry vision.\n",
      "*   **Slow-healing sores or cuts:** High blood sugar can impair blood flow and wound healing.\n",
      "*   **Fatigue:** Feeling tired and weak is a common symptom.\n",
      "*   **Nausea or vomiting:**  This can occur due to the body's response to high blood sugar.\n",
      "*   **Frequent infections:** High blood sugar can weaken the immune system, making you more susceptible to infections.\n",
      "*   **Darkened areas of skin (acanthosis nig\n"
     ]
    }
   ],
   "source": [
    "from app.components.llm import load_llm\n",
    "import torch\n",
    "\n",
    "tokenizer, model = load_llm()\n",
    "if not tokenizer or not model:\n",
    "    print(\"âŒ Failed to load Gemma LLM\")\n",
    "    exit()\n",
    "\n",
    "prompt = \"What are the symptoms of diabetes?\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=256)\n",
    "\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"LLM Response:\")\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a76a6e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Vector store missing\n"
     ]
    }
   ],
   "source": [
    "from app.components.vector_store import load_vector_store\n",
    "\n",
    "db = load_vector_store()\n",
    "if db:\n",
    "    print(\"âœ… Vector store loaded\")\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": 1})\n",
    "    docs = retriever.get_relevant_documents(\"diabetes symptoms\")\n",
    "    print(f\"Found {len(docs)} documents\")\n",
    "else:\n",
    "    print(\"âŒ Vector store missing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b58d791d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fq/8zxmzsmd2_3fkx4tw93_z4v00000gn/T/ipykernel_9085/947458774.py:24: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  res = qa_chain({\"query\": \"What are the symptoms of diabetes?\"})\n",
      "Python(44716) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'What are the symptoms of diabetes?', 'result': 'Dummy answer for testing'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms.base import LLM\n",
    "\n",
    "class DummyLLM(LLM):\n",
    "    @property\n",
    "    def _llm_type(self):\n",
    "        return \"dummy\"\n",
    "\n",
    "    def _call(self, prompt: str, stop=None):\n",
    "        return \"Dummy answer for testing\"\n",
    "\n",
    "from app.components.vector_store import load_vector_store\n",
    "\n",
    "db = load_vector_store()\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=DummyLLM(),\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=False,\n",
    ")\n",
    "\n",
    "res = qa_chain({\"query\": \"What are the symptoms of diabetes?\"})\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94e4c932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store loaded/created: None\n"
     ]
    }
   ],
   "source": [
    "from app.components.pdf_loader import load_pdf_files, create_text_chunks\n",
    "from app.components.vector_store import ensure_vector_store\n",
    "\n",
    "documents = load_pdf_files()\n",
    "chunks = create_text_chunks(documents)\n",
    "db = ensure_vector_store(chunks)\n",
    "print(\"Vector store loaded/created:\", db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d11c1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute path: /Users/nuzkhan/Downloads/medical chatbot/app/components/vectorstore/db_faiss\n",
      "Exists? True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from app.config.config import DB_FAISS_PATH\n",
    "\n",
    "print(\"Absolute path:\", os.path.abspath(DB_FAISS_PATH))\n",
    "print(\"Exists?\", os.path.exists(DB_FAISS_PATH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0376c4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute path: /Users/nuzkhan/Downloads/medical chatbot/app/components/vectorstore/db_faiss\n",
      "Exists? True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from app.config.config import DB_FAISS_PATH\n",
    "\n",
    "print(\"Absolute path:\", os.path.abspath(DB_FAISS_PATH))\n",
    "print(\"Exists?\", os.path.exists(DB_FAISS_PATH))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7f1635",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25821962",
   "metadata": {},
   "source": [
    "from app.components.pdf_loader import load_pdf_files, create_text_chunks\n",
    "\n",
    "docs = load_pdf_files()\n",
    "print(\"Documents loaded:\", len(docs))\n",
    "chunks = create_text_chunks(docs)\n",
    "print(\"Text chunks created:\", len(chunks))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9cc038",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69835ef8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff2f580",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffb4071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9f25d0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents loaded: 0\n",
      "Text chunks created: 0\n"
     ]
    }
   ],
   "source": [
    "from app.components.pdf_loader import load_pdf_files, create_text_chunks\n",
    "\n",
    "docs = load_pdf_files()\n",
    "print(\"Documents loaded:\", len(docs))\n",
    "chunks = create_text_chunks(docs)\n",
    "print(\"Text chunks created:\", len(chunks))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76aa8fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents loaded: 0\n"
     ]
    }
   ],
   "source": [
    "from app.components.pdf_loader import load_pdf_files\n",
    "\n",
    "docs = load_pdf_files()\n",
    "print(\"Documents loaded:\", len(docs))\n",
    "for d in docs:\n",
    "    print(d.metadata.get(\"source\"), d.page_content[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "958cc651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Vector store loaded successfully\n",
      "Number of documents: 7080\n"
     ]
    }
   ],
   "source": [
    "from app.components.vector_store import load_vector_store\n",
    "\n",
    "db = load_vector_store()\n",
    "if db:\n",
    "    print(\"âœ… Vector store loaded successfully\")\n",
    "    print(\"Number of documents:\", len(db.index_to_docstore_id))\n",
    "else:\n",
    "    print(\"âŒ Vector store missing\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f880a2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ QA chain creation failed\n"
     ]
    }
   ],
   "source": [
    "from app.components.retriever import create_qa_chain\n",
    "\n",
    "qa_chain = create_qa_chain()\n",
    "if qa_chain:\n",
    "    print(\"âœ… QA chain created\")\n",
    "    response = qa_chain({\"query\": \"What are the symptoms of diabetes?\"})\n",
    "    print(\"LLM Response:\", response[\"result\"])\n",
    "else:\n",
    "    print(\"âŒ QA chain creation failed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "793e9187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Gemma loaded successfully\n"
     ]
    }
   ],
   "source": [
    "from app.components.llm import load_llm\n",
    "\n",
    "tokenizer, model = load_llm()\n",
    "if tokenizer and model:\n",
    "    print(\"âœ… Gemma loaded successfully\")\n",
    "else:\n",
    "    print(\"âŒ Failed to load Gemma\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d84ed928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you?\n",
      "\n",
      "I'm a little confused about the concept of \"reverse engineering\" in a computer programming context. I've heard it often used in the news, and it seems to involve looking at a software program to understand how it works internally, rather\n"
     ]
    }
   ],
   "source": [
    "from app.components.llm import load_llm\n",
    "\n",
    "tokenizer, model = load_llm()\n",
    "prompt = \"Hello, how are you?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440a7fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707f3f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f243cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing QA chain...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "QA chain could not be created",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m qa_chain \u001b[38;5;241m=\u001b[39m create_qa_chain()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m qa_chain \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQA chain could not be created\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… QA chain ready\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Measure latency for each query\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: QA chain could not be created"
     ]
    }
   ],
   "source": [
    "# performance_evaluation.py\n",
    "\n",
    "import time\n",
    "import csv\n",
    "from app.components.retriever import create_qa_chain\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "TEST_QUERIES = [\n",
    "    \"What are the symptoms of diabetes?\",\n",
    "    \"What is hypertension?\",\n",
    "    \"What are the causes of asthma?\",\n",
    "    \"How is tuberculosis treated?\",\n",
    "    \"What are common side effects of insulin?\",\n",
    "]\n",
    "\n",
    "NUM_REPEAT = 10  # number of times to repeat each query for throughput measurement\n",
    "CSV_FILE = \"performance_results.csv\"\n",
    "\n",
    "# -----------------------------\n",
    "# Initialize QA chain\n",
    "# -----------------------------\n",
    "print(\"Initializing QA chain...\")\n",
    "qa_chain = create_qa_chain()\n",
    "if qa_chain is None:\n",
    "    raise RuntimeError(\"QA chain could not be created\")\n",
    "\n",
    "print(\"âœ… QA chain ready\\n\")\n",
    "\n",
    "# -----------------------------\n",
    "# Measure latency for each query\n",
    "# -----------------------------\n",
    "latency_results = []\n",
    "\n",
    "for query in TEST_QUERIES:\n",
    "    # Warm-up run\n",
    "    _ = qa_chain.invoke({\"query\": query})[\"result\"]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = qa_chain.invoke({\"query\": query})[\"result\"]\n",
    "    end_time = time.time()\n",
    "    \n",
    "    latency = end_time - start_time\n",
    "    latency_results.append({\"query\": query, \"latency_sec\": latency})\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"Result: {result}\")\n",
    "    print(f\"Latency: {latency:.3f} sec\\n\")\n",
    "\n",
    "# Average latency\n",
    "avg_latency = sum(r[\"latency_sec\"] for r in latency_results) / len(latency_results)\n",
    "print(f\"Average Latency: {avg_latency:.3f} sec\\n\")\n",
    "\n",
    "# -----------------------------\n",
    "# Measure throughput\n",
    "# -----------------------------\n",
    "num_queries = len(TEST_QUERIES) * NUM_REPEAT\n",
    "start_time = time.time()\n",
    "\n",
    "for _ in range(NUM_REPEAT):\n",
    "    for query in TEST_QUERIES:\n",
    "        _ = qa_chain.invoke({\"query\": query})[\"result\"]\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "throughput_per_sec = num_queries / total_time\n",
    "throughput_per_min = throughput_per_sec * 60\n",
    "\n",
    "print(f\"Total Queries: {num_queries}\")\n",
    "print(f\"Total Time: {total_time:.2f} sec\")\n",
    "print(f\"Throughput: {throughput_per_sec:.2f} req/sec, {throughput_per_min:.2f} req/min\\n\")\n",
    "\n",
    "# -----------------------------\n",
    "# Save results to CSV\n",
    "# -----------------------------\n",
    "with open(CSV_FILE, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"query\", \"latency_sec\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(latency_results)\n",
    "    writer.writerow({\"query\": \"Average\", \"latency_sec\": avg_latency})\n",
    "\n",
    "print(f\"âœ… Latency results saved to {CSV_FILE}\")\n",
    "print(\"Performance evaluation completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cb5f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medchatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
