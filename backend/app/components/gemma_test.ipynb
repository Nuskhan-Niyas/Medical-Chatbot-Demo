{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9be5018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/Users/nuzkhan/Downloads/medical chatbot/medchatbot/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Gemma 3 Notebook: Download and Run Instruction-Tuned 1B Model\n",
    "\n",
    "# 1️⃣ Install dependencies (run in a notebook cell)\n",
    "! pip install -U transformers accelerate bitsandbytes torch --quiet\n",
    "\n",
    "# 2️⃣ Import required libraries\n",
    "from transformers import pipeline, AutoTokenizer, BitsAndBytesConfig, Gemma3ForCausalLM\n",
    "import torch\n",
    "\n",
    "# 3️⃣ Choose model ID\n",
    "MODEL_ID = \"google/gemma-3-1b-it\"  # instruction-tuned 1B\n",
    "\n",
    "# 4️⃣ Set device (GPU if available)\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73c27d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Using pipeline API ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 24\u001b[0m\n\u001b[1;32m     10\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     11\u001b[0m     [\n\u001b[1;32m     12\u001b[0m         {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     ]\n\u001b[1;32m     21\u001b[0m ]\n\u001b[1;32m     23\u001b[0m output \u001b[38;5;241m=\u001b[39m pipe(messages, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline Output:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgenerated_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "# 5️⃣ Option 1: Use pipeline for quick testing\n",
    "print(\"=== Using pipeline API ===\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=MODEL_ID,\n",
    "    device=device,\n",
    "    torch_dtype=dtype\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"Explain the side effects of aspirin in simple terms.\"}]\n",
    "        },\n",
    "    ]\n",
    "]\n",
    "\n",
    "output = pipe(messages, max_new_tokens=100)\n",
    "print(\"Pipeline Output:\\n\", output[0]['generated_text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e907094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6️⃣ Option 2: Load model manually with quantization (memory-efficient)\n",
    "print(\"\\n=== Loading model manually with 8-bit quantization ===\")\n",
    "quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "model = Gemma3ForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=quant_config\n",
    ").eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "# Prepare inputs\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device).to(dtype)\n",
    "\n",
    "# Generate output\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "\n",
    "decoded = tokenizer.batch_decode(outputs)\n",
    "print(\"Manual Model Output:\\n\", decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca414652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/Users/nuzkhan/Downloads/medical chatbot/medchatbot/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "=== Pipeline API Test ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Output:\n",
      " [{'role': 'system', 'content': [{'type': 'text', 'text': \"You are a helpful medical assistant. Always include a disclaimer: 'This information is for educational purposes only and not a substitute for professional medical advice.'\"}]}, {'role': 'user', 'content': [{'type': 'text', 'text': 'What are the side effects of aspirin?'}]}, {'role': 'assistant', 'content': \"Okay, here’s information about the side effects of aspirin, presented in a helpful and informative way. **This information is for educational purposes only and not a substitute for professional medical advice.**  It’s important to discuss any concerns you have about aspirin with your doctor or pharmacist.\\n\\n**Side Effects of Aspirin**\\n\\nAspirin is a widely used medication with many benefits, but it can also cause side effects. It’s important to be aware of these potential issues, especially if you’re taking it regularly or have existing health conditions. Here's a breakdown of the common and less common side effects:\\n\\n**Common Side Effects (More Likely):**\\n\\n* **Stomach Upset:** This is the most frequently reported\"}]\n",
      "\n",
      "=== Manual Model Loading with 8-bit Quantization ===\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "The installed version of bitsandbytes (<0.43.1) requires CUDA, but CUDA is not available. You may need to install PyTorch with CUDA support or upgrade bitsandbytes to >=0.43.1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 67\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Manual Model Loading with 8-bit Quantization ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m quant_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(load_in_8bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 67\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGemma3ForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mMODEL_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     72\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL_ID)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Prepare inputs with chat template\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/medical chatbot/medchatbot/lib/python3.9/site-packages/transformers/modeling_utils.py:277\u001b[0m, in \u001b[0;36mrestore_default_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m~/Downloads/medical chatbot/medchatbot/lib/python3.9/site-packages/transformers/modeling_utils.py:4881\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4872\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m transformers_explicit_filename\u001b[38;5;241m.\u001b[39mendswith(\n\u001b[1;32m   4873\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4874\u001b[0m     ) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m transformers_explicit_filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors.index.json\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   4875\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   4876\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe transformers file in the config seems to be incorrect: it is neither a safetensors file \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4877\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(*.safetensors) nor a safetensors index file (*.safetensors.index.json): \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4878\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransformers_explicit_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4879\u001b[0m         )\n\u001b[0;32m-> 4881\u001b[0m hf_quantizer, config, dtype, device_map \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_quantizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_agent\u001b[49m\n\u001b[1;32m   4883\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4885\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4886\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   4887\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot combine Quantization and loading a model from a GGUF file, try again by making sure you did not passed a `quantization_config` or that you did not load a quantized model from the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4888\u001b[0m     )\n",
      "File \u001b[0;32m~/Downloads/medical chatbot/medchatbot/lib/python3.9/site-packages/transformers/quantizers/auto.py:319\u001b[0m, in \u001b[0;36mget_hf_quantizer\u001b[0;34m(config, quantization_config, dtype, from_tf, from_flax, device_map, weights_only, user_agent)\u001b[0m\n\u001b[1;32m    316\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_dtype(dtype)\n\u001b[1;32m    327\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n",
      "File \u001b[0;32m~/Downloads/medical chatbot/medchatbot/lib/python3.9/site-packages/transformers/quantizers/quantizer_bnb_8bit.py:85\u001b[0m, in \u001b[0;36mBnb8BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbitsandbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.43.1\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m---> 85\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     86\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe installed version of bitsandbytes (<0.43.1) requires CUDA, but CUDA is not available. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     87\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou may need to install PyTorch with CUDA support or upgrade bitsandbytes to >=0.43.1.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m         )\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m validate_bnb_backend_availability\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_bitsandbytes_multi_backend_available\n",
      "\u001b[0;31mImportError\u001b[0m: The installed version of bitsandbytes (<0.43.1) requires CUDA, but CUDA is not available. You may need to install PyTorch with CUDA support or upgrade bitsandbytes to >=0.43.1."
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Gemma 3 Medical Chatbot Notebook\n",
    "# =========================================\n",
    "\n",
    "# 1️⃣ Install required libraries (run once)\n",
    "! pip install -U transformers accelerate bitsandbytes torch --quiet\n",
    "\n",
    "# 2️⃣ Import required libraries\n",
    "from transformers import pipeline, AutoTokenizer, BitsAndBytesConfig, Gemma3ForCausalLM\n",
    "import torch\n",
    "\n",
    "# 3️⃣ Model ID for instruction-tuned 1B\n",
    "MODEL_ID = \"google/gemma-3-1b-it\"\n",
    "\n",
    "# 4️⃣ Set device (GPU if available)\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# 5️⃣ Define chat messages with medical disclaimer\n",
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"You are a helpful medical assistant. Always include a disclaimer: \"\n",
    "                            \"'This information is for educational purposes only and not a substitute for professional medical advice.'\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"What are the side effects of aspirin?\"}]\n",
    "        },\n",
    "    ]\n",
    "]\n",
    "\n",
    "# 6️⃣ Option 1: Quick test with pipeline API\n",
    "print(\"=== Pipeline API Test ===\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=MODEL_ID,\n",
    "    device=device,\n",
    "    torch_dtype=dtype\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Use the pipeline output correctly for chat-style inputs\n",
    "output = pipe(messages, max_new_tokens=150, temperature=0.7, top_p=0.9)\n",
    "\n",
    "# For instruction-tuned chat pipeline, the output is nested\n",
    "# Usually output[0][0]['generated_text']\n",
    "print(\"Pipeline Output:\\n\", output[0][0]['generated_text'])\n",
    "\n",
    "\n",
    "# 7️⃣ Option 2: Manual model loading with 8-bit quantization (memory-efficient)\n",
    "print(\"\\n=== Manual Model Loading with 8-bit Quantization ===\")\n",
    "quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "model = Gemma3ForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=quant_config\n",
    ").eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "# Prepare inputs with chat template\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device).to(dtype)\n",
    "\n",
    "# Generate output\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "decoded = tokenizer.batch_decode(outputs)\n",
    "print(\"Manual Model Output:\\n\", decoded)\n",
    "\n",
    "# =========================================\n",
    "# ✅ Notes:\n",
    "# - For longer answers, increase max_new_tokens\n",
    "# - Adjust temperature/top_p for creativity vs. safety\n",
    "# - For RAG integration: replace user prompt with retrieved medical data\n",
    "# =========================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56337a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cccea1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.3 is available.\n",
      "You should consider upgrading via the '/Users/nuzkhan/Downloads/medical chatbot/medchatbot/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "=== Pipeline API ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Output:\n",
      " [{'role': 'system', 'content': [{'type': 'text', 'text': \"You are a helpful medical assistant. Always include a disclaimer: 'This information is for educational purposes only and not a substitute for professional medical advice.'\"}]}, {'role': 'user', 'content': [{'type': 'text', 'text': 'What are the side effects of aspirin?'}]}, {'role': 'assistant', 'content': 'Okay, I can certainly help you with that! Here’s a breakdown of the potential side effects of aspirin, presented in a helpful and informative way. **Please remember, this is for educational purposes only and not a substitute for professional medical advice.** Always consult with your doctor or healthcare provider before taking any medication, including aspirin.\\n\\n**Common Side Effects (Generally Mild):**\\n\\n*   **Stomach Upset:** This is the most common side effect. It can include nausea, heartburn, indigestion, bloating, and abdominal pain.\\n*   **Increased Bleeding Risk:** This is the *most important* aspect to understand. Aspirin thins the blood, which can increase the risk of bleeding, especially if you’re taking other'}]\n",
      "\n",
      "=== Manual Model Loading (CPU) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to cast a BatchEncoding to type torch.float32. This is not supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Model Output:\n",
      " [\"<bos><start_of_turn>user\\nYou are a helpful medical assistant. Always include a disclaimer: 'This information is for educational purposes only and not a substitute for professional medical advice.'\\n\\nWhat are the side effects of aspirin?<end_of_turn>\\n<start_of_turn>model\\nOkay, I can certainly help you with that! Here’s a breakdown of the potential side effects of aspirin, presented in a way that’s helpful and includes a disclaimer:\\n\\n**Side Effects of Aspirin**\\n\\nAspirin, while a widely used medication, can have a range of side effects. It’s important to remember that not everyone experiences them, and the severity can vary. Here’s a breakdown of common and less common side effects:\\n\\n**Common Side Effects (Generally Mild):**\\n\\n*   **Upset Stomach/Heartburn:** This is probably the most frequently reported side effect. Aspirin can irritate the stomach lining, leading to indigestion, nausea, or heartburn.\\n*   **Bloating:** Some\"]\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Gemma 3 Medical Chatbot Notebook (Mac-friendly)\n",
    "# =========================================\n",
    "\n",
    "! pip install -U transformers accelerate torch --quiet\n",
    "\n",
    "from transformers import pipeline, AutoTokenizer, Gemma3ForCausalLM\n",
    "import torch\n",
    "\n",
    "MODEL_ID = \"google/gemma-3-1b-it\"\n",
    "\n",
    "# Device setup\n",
    "device = -1  # CPU (Mac does not have CUDA)\n",
    "dtype = torch.float32  # Use float32 on CPU\n",
    "\n",
    "# Messages with medical disclaimer\n",
    "messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"You are a helpful medical assistant. Always include a disclaimer: \"\n",
    "                            \"'This information is for educational purposes only and not a substitute for professional medical advice.'\"\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"What are the side effects of aspirin?\"}]\n",
    "        },\n",
    "    ]\n",
    "]\n",
    "\n",
    "# 1️⃣ Using pipeline (easier, Mac-compatible)\n",
    "print(\"=== Pipeline API ===\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=MODEL_ID,\n",
    "    device=device,\n",
    "    torch_dtype=dtype\n",
    ")\n",
    "\n",
    "output = pipe(messages, max_new_tokens=150, temperature=0.7, top_p=0.9)\n",
    "print(\"Pipeline Output:\\n\", output[0][0]['generated_text'])\n",
    "\n",
    "# 2️⃣ Manual model loading without CUDA\n",
    "print(\"\\n=== Manual Model Loading (CPU) ===\")\n",
    "model = Gemma3ForCausalLM.from_pretrained(MODEL_ID).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(dtype)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9\n",
    "    )\n",
    "\n",
    "decoded = tokenizer.batch_decode(outputs)\n",
    "print(\"Manual Model Output:\\n\", decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c728aa50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02743f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medchatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
